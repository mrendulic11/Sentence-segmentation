{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1vkItLOVV_o__YVegKo8EQ0qBcEOci-Zb","authorship_tag":"ABX9TyNBxtznXktjDiO7DUtp/Xxe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<center>\n","\n","# Natural language processing\n","## Project - Sentence segmentation\n","## Model training\n","\n","### 2023./2024.\n","## Matea Kunac, Marijana RenduliÄ‡\n","</center>"],"metadata":{"id":"7tyDR8oGgein"}},{"cell_type":"markdown","source":["# 1. Introduction"],"metadata":{"id":"UBRncPWY3WbF"}},{"cell_type":"markdown","source":["This notebook focuses on the model training for sentence segmentation."],"metadata":{"id":"NLFCxNoZ3YVF"}},{"cell_type":"markdown","source":["#2. Code"],"metadata":{"id":"WRpBQELM3i2s"}},{"cell_type":"markdown","source":["##Libraires"],"metadata":{"id":"ft385XBE3nqS"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hbMq5ZKuonqu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708615436829,"user_tz":-60,"elapsed":34354,"user":{"displayName":"Marijana","userId":"13940462459037984967"}},"outputId":"3169ab12-bf90-4674-f8e2-83a3a754c922"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import pickle\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from torch.nn.utils.rnn import pad_sequence\n","import os\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"],"metadata":{"id":"7Lurrj9fggSs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Functions"],"metadata":{"id":"eudSXW9H3pP2"}},{"cell_type":"code","source":["class TextChunkDataset(Dataset):\n","    \"\"\"\n","    Loads data chunks and their corresponding labels from specified pickle files\n","    \"\"\"\n","    def __init__(self, chunks_file, labels_file):\n","        with open(chunks_file, 'rb') as f:\n","            self.chunks = pickle.load(f)\n","        with open(labels_file, 'rb') as f:\n","            self.labels = pickle.load(f)\n","\n","    def __len__(self):\n","        return len(self.chunks)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.chunks[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float32)"],"metadata":{"id":"lCIDEe1YggYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiLayerBiGRUModel(nn.Module):\n","    \"\"\"\n","    multi-layer Bidirectional GRU architecture\n","\n","    The model consists of the following components:\n","    - Embedding Layer: Converts input tokens into dense vectors of a specified size (embedding_dim)\n","    - Multi-Layer BiGRU: Processes the embedded input sequentially in both forward and backward directions across multiple layers (num_layers)\n","    - Fully Connected (Linear) Layer: Transforms the BiGRU's output to the desired output dimension (output_dim)\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, num_layers=3):\n","        super(MultiLayerBiGRUModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Times 2 because it's bidirectional\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        output, hidden = self.gru(embedded)\n","        final_output = self.fc(output)\n","        return final_output"],"metadata":{"id":"rXxswlAX8_Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pad_collate(batch):\n","    \"\"\"\n","    Pads sequences to match the longest sequence in a batch\n","    \"\"\"\n","    (xx, yy) = zip(*batch)\n","\n","    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n","\n","    yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n","\n","    return xx_pad, yy_pad"],"metadata":{"id":"QJXumaItgqUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data_loader):\n","    \"\"\"\n","    A function to evaluate the model's performance on a given dataset.\n","    It switches the model to evaluation mode, computes predictions for the dataset,\n","    and calculates evaluation metrics such as accuracy, precision, recall, and F1 score.\n","    \"\"\"\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for chunks, labels in data_loader:\n","            chunks, labels = chunks.to(device), labels.to(device)\n","            outputs = model(chunks).squeeze(-1)\n","            predicted = torch.round(torch.sigmoid(outputs))\n","\n","            all_predictions.extend(predicted.view(-1).cpu().numpy())\n","            all_labels.extend(labels.view(-1).cpu().numpy())\n","\n","    all_predictions = np.array(all_predictions)\n","    all_labels = np.array(all_labels)\n","\n","    accuracy = 100 * (all_predictions == all_labels).mean()\n","    precision = precision_score(all_labels, all_predictions)\n","    recall = recall_score(all_labels, all_predictions)\n","    f1 = f1_score(all_labels, all_predictions)\n","\n","    return accuracy, precision, recall, f1"],"metadata":{"id":"WjFo_McXg-EW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Data loading"],"metadata":{"id":"e3W0oIHO3sZp"}},{"cell_type":"code","source":["train_dataset = TextChunkDataset('/content/drive/MyDrive/Sentence_segmentation_popravak/data/train_encoded_chunks.pkl', '/content/drive/MyDrive/Sentence_segmentation_popravak/data/train_encoded_labels.pkl')\n","dev_dataset = TextChunkDataset('/content/drive/MyDrive/Sentence_segmentation_popravak/data/dev_encoded_chunks.pkl', '/content/drive/MyDrive/Sentence_segmentation_popravak/data/dev_encoded_labels.pkl')\n","\n","batch_size = 64\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n","dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)"],"metadata":{"id":"G0nacZ88gxMh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_pickle(file_path):\n","    \"\"\"\n","    Load and return the contents of a pickle file.\n","    \"\"\"\n","    with open(file_path, 'rb') as f:\n","        return pickle.load(f)\n","\n","vocab = load_pickle('/content/drive/MyDrive/Sentence_segmentation_popravak/data/vocab.pkl')\n","\n","# Model parameters\n","vocab_size = len(vocab) + 1\n","embedding_dim = 100\n","hidden_dim = 128"],"metadata":{"id":"braD_grGhHFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Weighted"],"metadata":{"id":"BTDAhFLJ3unS"}},{"cell_type":"markdown","source":["- pos_weight Parameter: This parameter is used to weight the loss differently for the positive class, which can be beneficial when dealing with imbalanced datasets like the one that we have."],"metadata":{"id":"jEqxQDKc4NeE"}},{"cell_type":"code","source":["positive_labels = sum(label for sublist in train_dataset.labels for label in sublist if label == 1)\n","total_labels = sum(len(sublist) for sublist in train_dataset.labels)\n","negative_labels = total_labels - positive_labels\n","\n","print(positive_labels)\n","print(negative_labels)\n","\n","if positive_labels > 0:\n","    pos_weight = negative_labels / positive_labels\n","else:\n","    pos_weight = 1\n","\n","print(pos_weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ke_oAg4JnznB","executionInfo":{"status":"ok","timestamp":1708615490616,"user_tz":-60,"elapsed":355,"user":{"displayName":"Marijana","userId":"13940462459037984967"}},"outputId":"409c0c8f-a174-4917-febb-88361d93e20e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["19791\n","379141\n","19.157243191349604\n"]}]},{"cell_type":"markdown","source":["##Model training"],"metadata":{"id":"caWWkxoW4Tfp"}},{"cell_type":"code","source":["# Initialize the model\n","model = MultiLayerBiGRUModel(vocab_size, embedding_dim, hidden_dim)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","pos_weight_tensor = torch.tensor([pos_weight], device=device)\n","\n","criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n","\n","learning_rate = 0.001\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Early Stopping Parameters\n","patience = 10  # Number of epochs to wait for improvement before stopping\n","counter = 0  # Tracks how many epochs have gone by without improvement\n","best_f1_score = 0.0\n","early_stop = False\n","\n","model.to(device)\n","\n","best_f1_score = 0.0\n","best_model_state = None\n","model_save_dir = '/content/drive/MyDrive/Sentence_segmentation_popravak/models'\n","# Training loop\n","num_epochs = 100\n","for epoch in range(num_epochs):\n","    if early_stop:\n","        print(\"Early stopping triggered\")\n","        break\n","\n","    model.train()\n","    epoch_loss = 0  # Initialize epoch loss\n","    for i, (chunks, labels) in enumerate(train_loader):\n","        chunks, labels = chunks.to(device), labels.to(device)\n","\n","        # Forward pass\n","        predictions = model(chunks).squeeze(-1)\n","        loss = criterion(predictions, labels)\n","        epoch_loss += loss.item()  # Accumulate loss over the epoch\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Calculate average loss for the epoch\n","    avg_epoch_loss = epoch_loss / len(train_loader)\n","\n","    # Evaluate on dev set\n","    accuracy, precision, recall, f1 = evaluate(model, dev_loader)\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}')\n","\n","    # Check if current model is the best so far\n","    if f1 > best_f1_score:\n","        best_f1_score = f1\n","        counter = 0  # Reset counter\n","        best_model_state = model.state_dict()\n","        # Save the current best model\n","        best_model_path = os.path.join(model_save_dir, f'best_model_epoch_{epoch+1}_f1_{f1:.2f}.pth')\n","        torch.save(best_model_state, best_model_path)\n","    else:\n","        counter += 1\n","        if counter >= patience:\n","            early_stop = True\n","            print(f\"No improvement in {counter} epochs, stopping early.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pll2kxTMa4_d","executionInfo":{"status":"ok","timestamp":1708617252042,"user_tz":-60,"elapsed":1728726,"user":{"displayName":"Marijana","userId":"13940462459037984967"}},"outputId":"435482cc-5b5a-4464-8773-1303b5a6faaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/100], Loss: 0.4444, Accuracy: 98.91%, Precision: 0.86, Recall: 0.89, F1: 0.88\n","Epoch [2/100], Loss: 0.2122, Accuracy: 97.89%, Precision: 0.69, Recall: 0.91, F1: 0.79\n","Epoch [3/100], Loss: 0.1646, Accuracy: 98.42%, Precision: 0.77, Recall: 0.90, F1: 0.83\n","Epoch [4/100], Loss: 0.1209, Accuracy: 98.81%, Precision: 0.84, Recall: 0.89, F1: 0.87\n","Epoch [5/100], Loss: 0.0768, Accuracy: 98.77%, Precision: 0.84, Recall: 0.88, F1: 0.86\n","Epoch [6/100], Loss: 0.0456, Accuracy: 98.66%, Precision: 0.82, Recall: 0.88, F1: 0.85\n","Epoch [7/100], Loss: 0.0294, Accuracy: 98.96%, Precision: 0.89, Recall: 0.87, F1: 0.88\n","Epoch [8/100], Loss: 0.0202, Accuracy: 98.97%, Precision: 0.89, Recall: 0.87, F1: 0.88\n","Epoch [9/100], Loss: 0.0147, Accuracy: 99.03%, Precision: 0.90, Recall: 0.87, F1: 0.89\n","Epoch [10/100], Loss: 0.0118, Accuracy: 99.15%, Precision: 0.94, Recall: 0.86, F1: 0.90\n","Epoch [11/100], Loss: 0.0088, Accuracy: 99.07%, Precision: 0.92, Recall: 0.86, F1: 0.89\n","Epoch [12/100], Loss: 0.0073, Accuracy: 99.18%, Precision: 0.95, Recall: 0.85, F1: 0.90\n","Epoch [13/100], Loss: 0.0071, Accuracy: 99.11%, Precision: 0.94, Recall: 0.84, F1: 0.89\n","Epoch [14/100], Loss: 0.0060, Accuracy: 99.12%, Precision: 0.94, Recall: 0.85, F1: 0.89\n","Epoch [15/100], Loss: 0.0079, Accuracy: 98.90%, Precision: 0.88, Recall: 0.86, F1: 0.87\n","Epoch [16/100], Loss: 0.0064, Accuracy: 99.16%, Precision: 0.94, Recall: 0.86, F1: 0.90\n","Epoch [17/100], Loss: 0.0082, Accuracy: 99.05%, Precision: 0.91, Recall: 0.86, F1: 0.89\n","Epoch [18/100], Loss: 0.0049, Accuracy: 98.97%, Precision: 0.89, Recall: 0.87, F1: 0.88\n","Epoch [19/100], Loss: 0.0062, Accuracy: 99.05%, Precision: 0.93, Recall: 0.84, F1: 0.88\n","Epoch [20/100], Loss: 0.0057, Accuracy: 99.04%, Precision: 0.94, Recall: 0.83, F1: 0.88\n","Epoch [21/100], Loss: 0.0066, Accuracy: 99.11%, Precision: 0.92, Recall: 0.87, F1: 0.89\n","Epoch [22/100], Loss: 0.0041, Accuracy: 99.10%, Precision: 0.94, Recall: 0.84, F1: 0.89\n","No improvement in 10 epochs, stopping early.\n","Early stopping triggered\n"]}]}]}